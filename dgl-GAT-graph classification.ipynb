{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b423e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dgl.data import MiniGCDataset\n",
    "from dgl.nn.pytorch import *\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"true\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92258660",
   "metadata": {},
   "source": [
    "GAT层流程：\n",
    "1. 参数\n",
    "in_dim：输入的特征维度\n",
    "out_dim：输出的特征维度\n",
    "num_heads：多头注意力机制的头的数量\n",
    "feat_drop：特征丢弃概率（用于特征的丢弃, 也就是数据在传递过程中按照概率把一部分特征信息丢弃掉，让模型更加稳定而不容易过拟合）\n",
    "attn_drop：注意力丢弃概率（不同的是它是在正则化注意力机制的时候使用的。通过随机丢弃掉一些注意力信息，正则化模型，避免过拟合）\n",
    "alpha：激活函数的斜率\n",
    "edge_softmax：节点到边的 Softmax 函数\n",
    "agg_activation：节点到节点的聚合激活函数：elu\n",
    "【feat_drop：一般是设置为 0.6，也就是丢弃掉 60% 的特征信息；attn_drop：一般设置为 0.2；alpha：一般设置为 0.2.】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61fdcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 feat_drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 alpha=0.2,\n",
    "                 agg_activation=F.elu):\n",
    "        super(GATLayer, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.fc = nn.Linear(in_dim, num_heads * out_dim, bias=False)\n",
    "        self.attn_l = nn.Parameter(torch.Tensor(size=(num_heads, out_dim, 1)))\n",
    "        self.attn_r = nn.Parameter(torch.Tensor(size=(num_heads, out_dim, 1)))\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.activation = nn.LeakyReLU(alpha)\n",
    "        self.softmax = edge_softmax\n",
    "        self.agg_activation=agg_activation\n",
    "\n",
    "    def clean_data(self):\n",
    "        ndata_names = ['ft', 'a1', 'a2']\n",
    "        edata_names = ['a_drop']\n",
    "        for name in ndata_names:\n",
    "            self.g.ndata.pop(name)\n",
    "        for name in edata_names:\n",
    "            self.g.edata.pop(name)\n",
    "#         print(ndata_names)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # an edge UDF to compute un-normalized attention values from src and dst\n",
    "        a = self.activation(edges.src['a1'] + edges.dst['a2'])\n",
    "        return {'a' : a}\n",
    "\n",
    "    def edge_softmax(self):\n",
    "        attention = self.softmax(self.g, self.g.edata.pop('a'))\n",
    "        # Dropout attention scores and save them\n",
    "        self.g.edata['a_drop'] = self.attn_drop(attention)\n",
    "        \n",
    "        \n",
    "    def forward(self, feat, bg):\n",
    "        # prepare, inputs are of shape V x F, V the number of nodes, F the dim of input features\n",
    "        self.g = bg\n",
    "        h = self.feat_drop(feat)\n",
    "        \n",
    "        # V x K x F', K number of heads, F' dim of transformed features\n",
    "        ft = self.fc(h).reshape((h.shape[0], self.num_heads, -1))\n",
    "        head_ft = ft.transpose(0, 1)                              # K x V x F'\n",
    "        a1 = torch.bmm(head_ft, self.attn_l).transpose(0, 1)      # V x K x 1\n",
    "        a2 = torch.bmm(head_ft, self.attn_r).transpose(0, 1)      # V x K x 1\n",
    "        self.g.ndata.update({'ft' : ft, 'a1' : a1, 'a2' : a2})\n",
    "        # 1. compute edge attention\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # 2. compute softmax in two parts: exp(x - max(x)) and sum(exp(x - max(x)))\n",
    "        self.edge_softmax()\n",
    "        # 2. compute the aggregated node features scaled by the dropped,\n",
    "        # unnormalized attention values.\n",
    "        self.g.update_all(fn.u_mul_e('ft', 'a_drop', 'ft'), fn.sum('ft', 'ft'))\n",
    "        # 3. apply normalizer\n",
    "        ret = self.g.ndata['ft']                                  # V x K x F'\n",
    "        ret = ret.flatten(1)\n",
    "\n",
    "        if self.agg_activation is not None:\n",
    "            ret = self.agg_activation(ret)\n",
    "\n",
    "        # Clean ndata and edata\n",
    "        self.clean_data()\n",
    "\n",
    "        return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45fbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
    "        super(GATClassifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "#              初始化第一层 GATLayer\n",
    "            GATLayer(in_dim, hidden_dim, num_heads),\n",
    "#              初始化第二层 GATLayer\n",
    "            GATLayer(hidden_dim * num_heads, hidden_dim, num_heads)\n",
    "        ])\n",
    "#         分类\n",
    "        self.classify = nn.Linear(hidden_dim * num_heads, n_classes)\n",
    "\n",
    "    def forward(self, bg):\n",
    "        # For undirected graphs, in_degree is the same as out_degree.\n",
    "#          以 bg 中节点的入度为特征\n",
    "        h = bg.ndata[\"h\"].float()\n",
    "#     两层 GATLayer遍历\n",
    "        for i, gnn in enumerate(self.layers):\n",
    "#         将特征的值当作 GATLayer 层的输入\n",
    "            h = gnn(h, bg)\n",
    "#     把 GATLayer 层的输出作为新的特征\n",
    "        bg.ndata['h'] = h\n",
    "#     通过求均值，得到新特征的图表示\n",
    "        hg = dgl.mean_nodes(bg, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d8a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "class NTU(DGLDataset):\n",
    "    def __init__(self, raw_dir=None, force_reload=False, verbose=False):\n",
    "        super(NTU, self).__init__(name='NTU',\n",
    "                                          raw_dir=raw_dir,\n",
    "                                          force_reload=force_reload,\n",
    "                                          verbose=verbose)\n",
    "    def process(self):\n",
    "        src = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22,23,24]\n",
    "        dst = [1,20,20,2,20,4,5,6,20,8,9,10,0,12,13,14,0,16,17,18,22,7,24,11]\n",
    "        data_path = r\"C:/Users/YU TAO/Desktop/STGAT-main/prepare/ntu_60/ntu_60_new/xsub/val_data_joint.npy\"\n",
    "        label_path = \"C:/Users/YU TAO/Desktop/STGAT-main/prepare/ntu_60/ntu_60_new/xsub/val_label.pkl\"\n",
    "        # 处理标签\n",
    "        with open(label_path, 'rb') as f:\n",
    "            sample_name, label = pickle.load(f)\n",
    "        self.label = label\n",
    "    #     print(label.shape)\n",
    "        # 处理特征\n",
    "        ndata = np.load(data_path)\n",
    "        graphlist = []\n",
    "        for index, X in enumerate(ndata):\n",
    "            # 图创建\n",
    "            g = dgl.graph((src,dst))\n",
    "            g = dgl.to_bidirected(g)\n",
    "    #         print(index)\n",
    "    #         print(X.transpose(2,0,1,3).shape)\n",
    "            torch_X = torch.from_numpy(X.transpose(2,0,1,3))\n",
    "            g.ndata['h'] = torch_X\n",
    "            graphlist.append(g)\n",
    "        print(\"done\")\n",
    "#         print(dgl.batch（graphlist）)\n",
    "        self.graphs = graphlist\n",
    "#         print(self.graphs)\n",
    "#         print(self.graphs)\n",
    "       \n",
    "\n",
    "#     def save():\n",
    "#         \"\"\"save the graph list and the labels\"\"\"\n",
    "#         graphlist, label = load_graph()\n",
    "#         graph_path = os.path.join(\"C:/Users/YU TAO/Desktop/STGAT-main/prepare/ntu_60/ntu_60_new/\", 'dgl_graph.bin')\n",
    "#         save_graphs(str(graph_path), graphlist, {'labels': label})\n",
    "\n",
    "#     def has_cache():\n",
    "#         graph_path = os.path.join(self.save_path, 'dgl_graph.bin')\n",
    "#         return os.path.exists(graph_path)\n",
    "\n",
    "#     def load():\n",
    "#         graphs, label_dict = load_graph()\n",
    "#         self.graphs = graphs\n",
    "#         self.label = label_dict['labels']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_labels():\n",
    "        \"\"\"Number of labels for each graph, i.e. number of prediction tasks.\"\"\"\n",
    "        return 60\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r\"\"\" Get graph and label by index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (:class:`dgl.DGLGraph`, Tensor)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx], self.label[idx]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"Number of graphs in the dataset.\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        int\n",
    "        \"\"\"\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254e1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "#     print(samples)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "#     print(graphs)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd40ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (135000x2 and 1x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         bg \u001b[38;5;241m=\u001b[39m bg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m         label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 24\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#         print(prediction)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#         print(label)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mGATClassifier.forward\u001b[1;34m(self, bg)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     两层 GATLayer遍历\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, gnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         将特征的值当作 GATLayer 层的输入\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m             h \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#     把 GATLayer 层的输出作为新的特征\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         bg\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m h\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mGATLayer.forward\u001b[1;34m(self, feat, bg)\u001b[0m\n\u001b[0;32m     45\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_drop(feat)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# V x K x F', K number of heads, F' dim of transformed features\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     49\u001b[0m head_ft \u001b[38;5;241m=\u001b[39m ft\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)                              \u001b[38;5;66;03m# K x V x F'\u001b[39;00m\n\u001b[0;32m     50\u001b[0m a1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(head_ft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_l)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)      \u001b[38;5;66;03m# V x K x 1\u001b[39;00m\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (135000x2 and 1x256)"
     ]
    }
   ],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "dataset = NTU()\n",
    "\n",
    "# data_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "dataloader = GraphDataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate)\n",
    "# for iter, (bg, label) in enumerate(dataloader):\n",
    "#     print(bg)\n",
    "#     print(label)\n",
    "model = GATClassifier(1, 32, 8, 60)\n",
    "# print(model)\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for iter, (bg, label) in enumerate(dataloader):\n",
    "       \n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(bg)\n",
    "        prediction = prediction.to(device)\n",
    "#         print(prediction)\n",
    "#         print(label)\n",
    "        loss = loss_func(prediction, label)\n",
    "#         print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "        prediction = torch.softmax(model(bg), 1)\n",
    "        prediction = torch.multinomial(prediction, 1)\n",
    "#         print((prediction,label))\n",
    "        correct += (prediction == label.view(-1, 1)).sum().item()\n",
    "#         print(correct)\n",
    "#     print(iter)\n",
    "    acc = correct / ((iter + 1)*32)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}, Acc {:.4f}'.format(epoch, epoch_loss, acc))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "# model.eval()\n",
    "# print(next(model.parameters()).is_cuda)\n",
    "\n",
    "\n",
    "# # Convert a list of tuples to two lists\n",
    "# test_X, test_Y = map(list, zip(*testset))\n",
    "# test_bg = dgl.batch(test_X).to(device)\n",
    "# # print(test_bg.device)\n",
    "# test_Y = torch.tensor(test_Y).float().view(-1, 1).to(device)\n",
    "# probs_Y = torch.softmax(model(test_bg), 1).to(device)\n",
    "# # sampled_Y可能通过比argmax_Y预测更多的不同特征来估计类的潜在分布，而argmax_Y预测只考虑最大概率的类别\n",
    "\n",
    "# # torch.multinomial函数是从一组概率值中获取某一索引的函数。第一个参数probs_Y是一个概率矩阵，第二个参数1表示只从概率矩阵中抽取一次，所以结果中会返回一个索引。\n",
    "# sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "# # torch.max可以用来在一个张量中查找某个数值或者向量的最大值，\n",
    "# # 本例中的torch.max(probs_Y, 1)用于在probs_Y的每一行（即dim=1）中查找最大值，返回一个元组，元组的第一个元素为probs_Y每行的最大值，第二个元素为最大值的索引（即样本的预测类别）。\n",
    "# # [1].view(-1, 1)则用于将索引变换为与输入的Y样本同样的张量形状，即将其变换为列向量。\n",
    "# argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1).to(device)\n",
    "# print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "#     (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "# print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "#     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "\n",
    "# for graph, label in dataloader:\n",
    "#     print(graph)\n",
    "    \n",
    "# 图创建\n",
    "# g = dgl.DGLGraph((src,dst))\n",
    "# g = dgl.to_bidirected(g)\n",
    "# 无向图创建\n",
    "# graph = g.to_networkx().to_undirected()\n",
    "# 节点分布的方式，将各个节点的坐标通过字典的方式存储\n",
    "# pos = nx.kamada_kawai_layout(graph, center=[2,20])\n",
    "# pos[3]=[2,20.5]\n",
    "# pos[2]=[2.01,20.3]\n",
    "# pos[20]=[2.01,20.1]\n",
    "# pos[1]=[2.01,19.9]\n",
    "# pos[0]=[2.01,19.7]\n",
    "# print(pos)\n",
    "# options = {\"edgecolors\": \"tab:grey\", \"node_size\": 200, \"alpha\": 0.8, \"font_color\":\"whitesmoke\", \"font_size\":6, \"width\":1}\n",
    "# nx.draw(graph, pos, with_labels=True, node_color=\"tab:blue\",**options)\n",
    "# plt.show()\n",
    "# x = torch.randn(3, 25, 128, 2)\n",
    "# print(x.shape)\n",
    "# 维度交换\n",
    "# print(x.permute(1,0,2,3).shape)\n",
    "# g.ndata['f'] = x.permute(1,0,2,3)\n",
    "# print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a335e10",
   "metadata": {},
   "source": [
    "collate用于将不同大小的数据集合并为统一格式（将图数据和标签数据分成两个list），它主要用于打包可变大小的输入以构建小批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate(samples):\n",
    "#     # The input `samples` is a list of pairs\n",
    "#     #  (graph, label).\n",
    "# #     print(samples)\n",
    "#     graphs, labels = map(list, zip(*samples))\n",
    "#     batched_graph = dgl.batch(graphs)\n",
    "# #     print(batched_graph)\n",
    "#     return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0903fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets.\n",
    "# dgl.data.MiniGCDataset(num_graphs, min_num_v, max_num_v, seed=0, save_graph=True, force_reload=False, verbose=False, transform=None)\n",
    "trainset = MiniGCDataset(320, 10, 20)\n",
    "testset = MiniGCDataset(80, 10, 20)\n",
    "\n",
    "print(trainset)\n",
    "\n",
    "# Use PyTorch's DataLoader and the collate function\n",
    "# defined before.\n",
    "data_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "# print(data_loader)\n",
    "# fn参数：def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
    "model = GATClassifier(1, 32, 8, trainset.num_classes)\n",
    "# print(model)\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for iter, (bg, label) in enumerate(data_loader):\n",
    "       \n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(bg)\n",
    "        prediction = prediction.to(device)\n",
    "#         print(prediction)\n",
    "#         print(label)\n",
    "        loss = loss_func(prediction, label)\n",
    "#         print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "        prediction = torch.softmax(model(bg), 1)\n",
    "        prediction = torch.multinomial(prediction, 1)\n",
    "#         print((prediction,label))\n",
    "        correct += (prediction == label.view(-1, 1)).sum().item()\n",
    "#         print(correct)\n",
    "#     print(iter)\n",
    "    acc = correct / ((iter + 1)*32)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}, Acc {:.4f}'.format(epoch, epoch_loss, acc))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "model.eval()\n",
    "# print(next(model.parameters()).is_cuda)\n",
    "\n",
    "\n",
    "# Convert a list of tuples to two lists\n",
    "test_X, test_Y = map(list, zip(*testset))\n",
    "test_bg = dgl.batch(test_X).to(device)\n",
    "# print(test_bg.device)\n",
    "test_Y = torch.tensor(test_Y).float().view(-1, 1).to(device)\n",
    "probs_Y = torch.softmax(model(test_bg), 1).to(device)\n",
    "# sampled_Y可能通过比argmax_Y预测更多的不同特征来估计类的潜在分布，而argmax_Y预测只考虑最大概率的类别\n",
    "\n",
    "# torch.multinomial函数是从一组概率值中获取某一索引的函数。第一个参数probs_Y是一个概率矩阵，第二个参数1表示只从概率矩阵中抽取一次，所以结果中会返回一个索引。\n",
    "sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "# torch.max可以用来在一个张量中查找某个数值或者向量的最大值，\n",
    "# 本例中的torch.max(probs_Y, 1)用于在probs_Y的每一行（即dim=1）中查找最大值，返回一个元组，元组的第一个元素为probs_Y每行的最大值，第二个元素为最大值的索引（即样本的预测类别）。\n",
    "# [1].view(-1, 1)则用于将索引变换为与输入的Y样本同样的张量形状，即将其变换为列向量。\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1).to(device)\n",
    "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "    (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea4a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
