{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71b423e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dgl.data import MiniGCDataset\n",
    "from dgl.nn.pytorch import *\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"true\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92258660",
   "metadata": {},
   "source": [
    "GAT层流程：\n",
    "1. 参数\n",
    "in_dim：输入的特征维度\n",
    "out_dim：输出的特征维度\n",
    "num_heads：多头注意力机制的头的数量\n",
    "feat_drop：特征丢弃概率（用于特征的丢弃, 也就是数据在传递过程中按照概率把一部分特征信息丢弃掉，让模型更加稳定而不容易过拟合）\n",
    "attn_drop：注意力丢弃概率（不同的是它是在正则化注意力机制的时候使用的。通过随机丢弃掉一些注意力信息，正则化模型，避免过拟合）\n",
    "alpha：激活函数的斜率\n",
    "edge_softmax：节点到边的 Softmax 函数\n",
    "agg_activation：节点到节点的聚合激活函数：elu\n",
    "【feat_drop：一般是设置为 0.6，也就是丢弃掉 60% 的特征信息；attn_drop：一般设置为 0.2；alpha：一般设置为 0.2.】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61fdcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 feat_drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 alpha=0.2,\n",
    "                 agg_activation=F.elu):\n",
    "        super(GATLayer, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.fc = nn.Linear(in_dim, num_heads * out_dim, bias=False)\n",
    "        print(self.fc)\n",
    "        self.attn_l = nn.Parameter(torch.Tensor(size=(num_heads, out_dim * 900, 1)))\n",
    "        self.attn_r = nn.Parameter(torch.Tensor(size=(num_heads, out_dim * 900, 1)))\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.activation = nn.LeakyReLU(alpha)\n",
    "        self.softmax = edge_softmax\n",
    "        self.agg_activation=agg_activation\n",
    "\n",
    "    def clean_data(self):\n",
    "        ndata_names = ['h', 'a1', 'a2']\n",
    "        edata_names = ['a_drop']\n",
    "        for name in ndata_names:\n",
    "            self.g.ndata.pop(name)\n",
    "        for name in edata_names:\n",
    "            self.g.edata.pop(name)\n",
    "#         print(ndata_names)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # an edge UDF to compute un-normalized attention values from src and dst\n",
    "        a = self.activation(edges.src['a1'] + edges.dst['a2'])\n",
    "        return {'a' : a}\n",
    "\n",
    "    def edge_softmax(self):\n",
    "        attention = self.softmax(self.g, self.g.edata.pop('a'))\n",
    "        # Dropout attention scores and save them\n",
    "        self.g.edata['a_drop'] = self.attn_drop(attention)\n",
    "        \n",
    "        \n",
    "    def forward(self, feat, bg):\n",
    "        # prepare, inputs are of shape V x F, V the number of nodes, F the dim of input features\n",
    "        self.g = bg\n",
    "        h = self.feat_drop(feat)\n",
    "        print(\"h\")\n",
    "        print(h.shape)\n",
    "        # V x K x F', K number of heads, F' dim of transformed features\n",
    "        ft = self.fc(h).reshape((h.shape[0], self.num_heads, -1))\n",
    "        print(\"self.fc\")\n",
    "        print(self.fc(h).shape)\n",
    "        print(\"fc\")\n",
    "        print(ft.shape)\n",
    "        head_ft = ft.transpose(0, 1)                              # K x V x F'\n",
    "#         print(\"222\")\n",
    "        print(\"head_ft\")\n",
    "        print(head_ft.shape)\n",
    "        print(\"attn_l\")\n",
    "        print(self.attn_l.shape)\n",
    "        a1 = torch.bmm(head_ft, self.attn_l).transpose(0, 1)      # V x K x 1\n",
    "        print(a1.shape)\n",
    "        a2 = torch.bmm(head_ft, self.attn_r).transpose(0, 1)      # V x K x 1\n",
    "        print(a2.shape)\n",
    "        self.g.ndata.update({'h' : ft, 'a1' : a1, 'a2' : a2})\n",
    "        print(self.g)\n",
    "        # 1. compute edge attention\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # 2. compute softmax in two parts: exp(x - max(x)) and sum(exp(x - max(x)))\n",
    "        self.edge_softmax()\n",
    "        # 2. compute the aggregated node features scaled by the dropped,\n",
    "        # unnormalized attention values.\n",
    "        self.g.update_all(fn.u_mul_e('h', 'a_drop', 'h'), fn.sum('h', 'h'))\n",
    "#         print(self.g)\n",
    "        # 3. apply normalizer\n",
    "        ret = self.g.ndata['h']                                  # V x K x F'\n",
    "#         print(ret.shape)\n",
    "        ret = ret.flatten(1)\n",
    "\n",
    "        if self.agg_activation is not None:\n",
    "            ret = self.agg_activation(ret)\n",
    "\n",
    "        # Clean ndata and edata\n",
    "        self.clean_data()\n",
    "#         print(ret)\n",
    "        return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b45fbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
    "        super(GATClassifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "#              初始化第一层 GATLayer\n",
    "            GATLayer(in_dim, hidden_dim, num_heads),\n",
    "#              初始化第二层 GATLayer\n",
    "            GATLayer(hidden_dim * num_heads, hidden_dim, num_heads)\n",
    "        ])\n",
    "#         分类\n",
    "        self.classify = nn.Linear(hidden_dim * num_heads, n_classes)\n",
    "\n",
    "    def forward(self, bg):\n",
    "        # For undirected graphs, in_degree is the same as out_degree.\n",
    "#          以 bg 中节点的入度为特征\n",
    "        h = bg.ndata[\"h\"].float()\n",
    "#         print(h.shape)\n",
    "#     两层 GATLayer遍历\n",
    "        for i, gnn in enumerate(self.layers):\n",
    "#         将特征的值当作 GATLayer 层的输入\n",
    "            h = gnn(h, bg)\n",
    "            print(h)\n",
    "#     把 GATLayer 层的输出作为新的特征\n",
    "        bg.ndata['h'] = h\n",
    "#         print(bg.ndata['h'])\n",
    "#     通过求均值，得到新特征的图表示\n",
    "        hg = dgl.mean_nodes(bg, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e5d8a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "class NTU(DGLDataset):\n",
    "    def __init__(self, raw_dir=None, force_reload=False, verbose=False):\n",
    "        super(NTU, self).__init__(name='NTU',\n",
    "                                          raw_dir=raw_dir,\n",
    "                                          force_reload=force_reload,\n",
    "                                          verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        src = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22,23,24]\n",
    "        dst = [1,20,20,2,20,4,5,6,20,8,9,10,0,12,13,14,0,16,17,18,22,7,24,11]\n",
    "        windows = 5\n",
    "        for i in range(windows-1):\n",
    "        #     print(i)\n",
    "            src_tensor = torch.tensor(src[-24:])\n",
    "            dst_tensor = torch.tensor(dst[-24:])\n",
    "            flg = np.random.randint((i+1)*25+2,(i+1)*25+3,25)\n",
    "            dst+=src[-24:]\n",
    "            src+=flg.tolist()\n",
    "        #     print((src_tensor+25).tolist())\n",
    "            src+=(src_tensor+25).tolist()\n",
    "            dst+=(dst_tensor+25).tolist()\n",
    "        \n",
    "        data_path = r\"C:/Users/YU TAO/Desktop/STGAT-main/prepare/ntu_60/ntu_60_new/xsub/val_data_joint.npy\"\n",
    "        label_path = \"C:/Users/YU TAO/Desktop/STGAT-main/prepare/ntu_60/ntu_60_new/xsub/val_label.pkl\"\n",
    "        # 处理标签\n",
    "        with open(label_path, 'rb') as f:\n",
    "            sample_name, label = pickle.load(f)\n",
    "        self.label = label\n",
    "    #     print(label.shape)\n",
    "        # 处理特征\n",
    "        ndata = np.load(data_path)\n",
    "        graphlist = []\n",
    "        for index, X in enumerate(ndata):\n",
    "            # 图创建\n",
    "            g = dgl.graph((src,dst))\n",
    "            g = dgl.to_bidirected(g)\n",
    "    #         print(index)\n",
    "    #         print(X.transpose(2,0,1,3).shape)\n",
    "            torch_X = torch.from_numpy(X.transpose(2,0,1,3))\n",
    "            g.ndata['h'] = torch_X\n",
    "            graphlist.append(g)\n",
    "        print(\"done\")\n",
    "#         print(dgl.batch（graphlist）)\n",
    "        self.graphs = graphlist\n",
    "#         print(self.graphs)\n",
    "#         print(self.graphs)\n",
    "       \n",
    "\n",
    "#     def save():\n",
    "#         \"\"\"save the graph list and the labels\"\"\"\n",
    "#         graphlist, label = load_graph()\n",
    "#         graph_path = os.path.join(\"C:/Users/YU TAO/Desktop/STGAT-main/prepare/ntu_60/ntu_60_new/\", 'dgl_graph.bin')\n",
    "#         save_graphs(str(graph_path), graphlist, {'labels': label})\n",
    "\n",
    "#     def has_cache():\n",
    "#         graph_path = os.path.join(self.save_path, 'dgl_graph.bin')\n",
    "#         return os.path.exists(graph_path)\n",
    "\n",
    "#     def load():\n",
    "#         graphs, label_dict = load_graph()\n",
    "#         self.graphs = graphs\n",
    "#         self.label = label_dict['labels']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_labels():\n",
    "        \"\"\"Number of labels for each graph, i.e. number of prediction tasks.\"\"\"\n",
    "        return 60\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r\"\"\" Get graph and label by index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (:class:`dgl.DGLGraph`, Tensor)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx], self.label[idx]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"Number of graphs in the dataset.\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        int\n",
    "        \"\"\"\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "254e1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label). \n",
    "#     print(samples)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "#     print(graphs)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7cd40ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "[01:43:32] C:\\Users\\Administrator\\dgl-0.5\\src\\graph\\unit_graph.cc:71: Check failed: src->shape[0] == dst->shape[0] (220 vs. 216) : Input arrays should have the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphDataLoader\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNTU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# data_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m GraphDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate)\n",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36mNTU.__init__\u001b[1;34m(self, raw_dir, force_reload, verbose)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNTU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNTU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mraw_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\data\\dgl_dataset.py:113\u001b[0m, in \u001b[0;36mDGLDataset.__init__\u001b[1;34m(self, name, url, raw_dir, save_dir, hash_key, force_reload, verbose, transform)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_dir \u001b[38;5;241m=\u001b[39m save_dir\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\data\\dgl_dataset.py:204\u001b[0m, in \u001b[0;36mDGLDataset._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m load_flag:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36mNTU.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m     graphlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, X \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ndata):\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# 图创建\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         g \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m         g \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mto_bidirected(g)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#         print(index)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#         print(X.transpose(2,0,1,3).shape)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\convert.py:164\u001b[0m, in \u001b[0;36mgraph\u001b[1;34m(data, num_nodes, idtype, device, row_sorted, col_sorted)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe num_nodes argument must be larger than the max ID in the data,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    161\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_nodes, \u001b[38;5;28mmax\u001b[39m(urange, vrange) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    162\u001b[0m     urange, vrange \u001b[38;5;241m=\u001b[39m num_nodes, num_nodes\n\u001b[1;32m--> 164\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_from_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_fmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_N\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_E\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_N\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvrange\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mrow_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_sorted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_sorted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\convert.py:1718\u001b[0m, in \u001b[0;36mcreate_from_edges\u001b[1;34m(sparse_fmt, arrays, utype, etype, vtype, urange, vrange, row_sorted, col_sorted)\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1717\u001b[0m     u, v \u001b[38;5;241m=\u001b[39m arrays\n\u001b[1;32m-> 1718\u001b[0m     hgidx \u001b[38;5;241m=\u001b[39m \u001b[43mheterograph_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_unitgraph_from_coo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_ntypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvrange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_sorted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_sorted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# 'csr' or 'csc'\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m     indptr, indices, eids \u001b[38;5;241m=\u001b[39m arrays\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\heterograph_index.py:1245\u001b[0m, in \u001b[0;36mcreate_unitgraph_from_coo\u001b[1;34m(num_ntypes, num_src, num_dst, row, col, formats, row_sorted, col_sorted)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formats, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1244\u001b[0m     formats \u001b[38;5;241m=\u001b[39m [formats]\n\u001b[1;32m-> 1245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_CAPI_DGLHeteroCreateUnitGraphFromCOO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_ntypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_src\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_dst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dgl_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dgl_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py:213\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    211\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m DGLValue()\n\u001b[0;32m    212\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m--> 213\u001b[0m \u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDGLFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[0;32m    224\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32mC:\\softwrae\\Anaconda\\envs\\torch\\lib\\site-packages\\dgl\\_ffi\\base.py:70\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(py_str(_LIB\u001b[38;5;241m.\u001b[39mDGLGetLastError()))\n",
      "\u001b[1;31mDGLError\u001b[0m: [01:43:32] C:\\Users\\Administrator\\dgl-0.5\\src\\graph\\unit_graph.cc:71: Check failed: src->shape[0] == dst->shape[0] (220 vs. 216) : Input arrays should have the same length."
     ]
    }
   ],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "dataset = NTU()\n",
    "\n",
    "# data_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "dataloader = GraphDataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate)\n",
    "# for iter, (bg, label) in enumerate(dataloader):\n",
    "#     print(bg)\n",
    "#     print(label)\n",
    "model = GATClassifier(2, 16, 8, 60)\n",
    "# print(model)\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for iter, (bg, label) in enumerate(dataloader):\n",
    "       \n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "#         print(bg)\n",
    "        prediction = model(bg)\n",
    "#         print(prediction)\n",
    "        prediction = prediction.to(device)\n",
    "#         print(label)\n",
    "        loss = loss_func(prediction, label)\n",
    "#         print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "        prediction = torch.softmax(model(bg), 1)\n",
    "        prediction = torch.multinomial(prediction, 1)\n",
    "#         print((prediction,label))\n",
    "        correct += (prediction == label.view(-1, 1)).sum().item()\n",
    "#         print(correct)\n",
    "#     print(iter)\n",
    "    acc = correct / ((iter + 1)*32)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}, Acc {:.4f}'.format(epoch, epoch_loss, acc))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "# model.eval()\n",
    "# print(next(model.parameters()).is_cuda)\n",
    "\n",
    "\n",
    "# # Convert a list of tuples to two lists\n",
    "# test_X, test_Y = map(list, zip(*testset))\n",
    "# test_bg = dgl.batch(test_X).to(device)\n",
    "# # print(test_bg.device)\n",
    "# test_Y = torch.tensor(test_Y).float().view(-1, 1).to(device)\n",
    "# probs_Y = torch.softmax(model(test_bg), 1).to(device)\n",
    "# # sampled_Y可能通过比argmax_Y预测更多的不同特征来估计类的潜在分布，而argmax_Y预测只考虑最大概率的类别\n",
    "\n",
    "# # torch.multinomial函数是从一组概率值中获取某一索引的函数。第一个参数probs_Y是一个概率矩阵，第二个参数1表示只从概率矩阵中抽取一次，所以结果中会返回一个索引。\n",
    "# sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "# # torch.max可以用来在一个张量中查找某个数值或者向量的最大值，\n",
    "# # 本例中的torch.max(probs_Y, 1)用于在probs_Y的每一行（即dim=1）中查找最大值，返回一个元组，元组的第一个元素为probs_Y每行的最大值，第二个元素为最大值的索引（即样本的预测类别）。\n",
    "# # [1].view(-1, 1)则用于将索引变换为与输入的Y样本同样的张量形状，即将其变换为列向量。\n",
    "# argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1).to(device)\n",
    "# print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "#     (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "# print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "#     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "\n",
    "# for graph, label in dataloader:\n",
    "#     print(graph)\n",
    "    \n",
    "# 图创建\n",
    "# g = dgl.DGLGraph((src,dst))\n",
    "# g = dgl.to_bidirected(g)\n",
    "# 无向图创建\n",
    "# graph = g.to_networkx().to_undirected()\n",
    "# 节点分布的方式，将各个节点的坐标通过字典的方式存储\n",
    "# pos = nx.kamada_kawai_layout(graph, center=[2,20])\n",
    "# pos[3]=[2,20.5]\n",
    "# pos[2]=[2.01,20.3]\n",
    "# pos[20]=[2.01,20.1]\n",
    "# pos[1]=[2.01,19.9]\n",
    "# pos[0]=[2.01,19.7]\n",
    "# print(pos)\n",
    "# options = {\"edgecolors\": \"tab:grey\", \"node_size\": 200, \"alpha\": 0.8, \"font_color\":\"whitesmoke\", \"font_size\":6, \"width\":1}\n",
    "# nx.draw(graph, pos, with_labels=True, node_color=\"tab:blue\",**options)\n",
    "# plt.show()\n",
    "# x = torch.randn(3, 25, 128, 2)\n",
    "# print(x.shape)\n",
    "# 维度交换\n",
    "# print(x.permute(1,0,2,3).shape)\n",
    "# g.ndata['f'] = x.permute(1,0,2,3)\n",
    "# print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a335e10",
   "metadata": {},
   "source": [
    "collate用于将不同大小的数据集合并为统一格式（将图数据和标签数据分成两个list），它主要用于打包可变大小的输入以构建小批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "#     print(samples)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "#     print(batched_graph)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0903fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets.\n",
    "# dgl.data.MiniGCDataset(num_graphs, min_num_v, max_num_v, seed=0, save_graph=True, force_reload=False, verbose=False, transform=None)\n",
    "trainset = MiniGCDataset(320, 10, 20)\n",
    "testset = MiniGCDataset(80, 10, 20)\n",
    "\n",
    "# print(trainset)\n",
    "\n",
    "# Use PyTorch's DataLoader and the collate function\n",
    "# defined before.\n",
    "data_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "\n",
    "# print(data_loader)\n",
    "# fn参数：def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
    "model = GATClassifier(1, 32, 8, trainset.num_classes)\n",
    "# print(model)\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for iter, (bg, label) in enumerate(data_loader):\n",
    "       \n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "#         print(bg.shape)\n",
    "        prediction = model(bg)\n",
    "        prediction = prediction.to(device)\n",
    "#         print(prediction)\n",
    "#         print(label)\n",
    "        loss = loss_func(prediction, label)\n",
    "#         print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        \n",
    "        prediction = torch.softmax(model(bg), 1)\n",
    "        prediction = torch.multinomial(prediction, 1)\n",
    "#         print((prediction,label))\n",
    "        correct += (prediction == label.view(-1, 1)).sum().item()\n",
    "#         print(correct)\n",
    "#     print(iter)\n",
    "    acc = correct / ((iter + 1)*32)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}, Acc {:.4f}'.format(epoch, epoch_loss, acc))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "model.eval()\n",
    "# print(next(model.parameters()).is_cuda)\n",
    "\n",
    "\n",
    "# Convert a list of tuples to two lists\n",
    "test_X, test_Y = map(list, zip(*testset))\n",
    "test_bg = dgl.batch(test_X).to(device)\n",
    "# print(test_bg.device)\n",
    "test_Y = torch.tensor(test_Y).float().view(-1, 1).to(device)\n",
    "probs_Y = torch.softmax(model(test_bg), 1).to(device)\n",
    "# sampled_Y可能通过比argmax_Y预测更多的不同特征来估计类的潜在分布，而argmax_Y预测只考虑最大概率的类别\n",
    "\n",
    "# torch.multinomial函数是从一组概率值中获取某一索引的函数。第一个参数probs_Y是一个概率矩阵，第二个参数1表示只从概率矩阵中抽取一次，所以结果中会返回一个索引。\n",
    "sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "# torch.max可以用来在一个张量中查找某个数值或者向量的最大值，\n",
    "# 本例中的torch.max(probs_Y, 1)用于在probs_Y的每一行（即dim=1）中查找最大值，返回一个元组，元组的第一个元素为probs_Y每行的最大值，第二个元素为最大值的索引（即样本的预测类别）。\n",
    "# [1].view(-1, 1)则用于将索引变换为与输入的Y样本同样的张量形状，即将其变换为列向量。\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1).to(device)\n",
    "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "    (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea4a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
